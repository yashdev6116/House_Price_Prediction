# -*- coding: utf-8 -*-
"""Housepriceprediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xpo72DO5-RVxRUe0MVqiuy-ItqhAyUZc
"""

!pip install pandas numpy matplotlib seaborn scikit-learn

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, GradientBoostingRegressor
from sklearn.feature_selection import RFE
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.inspection import permutation_importance
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV

from google.colab import files
upload=files.upload()

housing_data = pd.read_csv('/content/Housing.csv')

print(housing_data.columns)

housing_data.info()

duplicate_rows = housing_data[housing_data.duplicated()]
if duplicate_rows.empty:
    print("No duplicate rows found.")
else:
    print("Duplicate rows found.")

housing_data.describe().T

print(housing_data.isnull().sum())
sns.histplot(data=housing_data, x='area')
plt.show()

fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))

# Ploting the first countplot on the first subplot (axs[0])
sns.countplot(x=housing_data["airconditioning"], hue=housing_data["stories"], palette="icefire", ax=axs[0])

# Ploting the second countplot on the second subplot (axs[1])
sns.countplot(x=housing_data["hotwaterheating"], hue=housing_data["stories"], palette="icefire", ax=axs[1])

axs[0].set_title("Air conditioning")
axs[1].set_title("Hot water heating")

plt.figure(figsize=(12, 6))

sns.barplot(y=housing_data["stories"].index,x=housing_data["basement"],palette="icefire")

housing_data["mainroad"].value_counts()

# Split data into features and target
X = housing_data.drop('price', axis=1)
y = housing_data['price']

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Identify categorical and numerical columns
categorical_cols = X_train.select_dtypes(include=['object']).columns
numerical_cols = X_train.select_dtypes(exclude=['object']).columns

# Create a ColumnTransformer
ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), categorical_cols)], remainder='passthrough')

# Transform the training and testing data
X_train = ct.fit_transform(X_train)
X_test = ct.transform(X_test)

# Feature Selection
model = LinearRegression()
rfe = RFE(model, n_features_to_select=10)
rfe.fit(X_train, y_train)
selected_features = np.flatnonzero(rfe.support_)
X_train = X_train[:, selected_features]
X_test = X_test[:, selected_features]

# Model Comparison
models = {
    'Linear Regression': LinearRegression(n_jobs=5),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting':GradientBoostingRegressor(),
    'Ridge Regression': Ridge(),
    "SVC":SVC()
}
for model in models.items():
    print(model[0])

a=1
best_r2=-1
for name, model in models.items():
    pipe = Pipeline([
        ('scaler', StandardScaler()),
        ('model', model)
    ])


    model=pipe.fit(X_train, y_train)
    y_pre=model.predict(X_test)

    print(f'{a} :- {name} - MSE: {mean_squared_error(y_test, y_pre):.4f}')
    print(f'     {name} - R2: {r2_score(y_test, y_pre):.4f}')
    print(f'     {name} - MAE: {mean_absolute_error(y_test, y_pre):.4f}',"\n")
    a+=1
    r2=r2_score(y_test, y_pre)
    if r2 > best_r2:
        best_r2 = r2
        best_model_name = name
        best_model = model

print(f'The best model is: {best_model_name}')
print(f'The best model R2 Score is: {best_r2}')

# Cross-Validation
model = RandomForestRegressor()
scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')
print(f"Cross-validation scores: {scores}")

# Regularization
ridge = Ridge(alpha=0.5)
ridge.fit(X_train, y_train)

lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)

# Create the model object
model = RandomForestRegressor()

# Fit the model to the training data
model.fit(X_train, y_train)

# Now we can make predictions on the test data!!!
y_pred = model.predict(X_test)
residuals = y_test - y_pred

# Plot residuals
plt.scatter(y_pred, residuals)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.show()

# Ensemble Methods
bag_model = BaggingRegressor(base_estimator=DecisionTreeRegressor())
bag_model.fit(X_train, y_train)

gbr_model = GradientBoostingRegressor()
gbr_model.fit(X_train, y_train)

print("Length of transformed_feature_names:", len(transformed_feature_names))
print("Length of importances:", len(importances))

# Filtering out the feature names that don't have corresponding importances
transformed_feature_names = transformed_feature_names[:len(importances)]

# Plot feature importances
plt.figure(figsize=(10, 6))
plt.bar(transformed_feature_names, importances)
plt.xticks(rotation=90)
plt.xlabel('Features')
plt.ylabel('Importance')
plt.title('Feature Importances')
plt.show()

# Data to predict on
data_to_predict = [
    [6000, 3, 2, 2, 1, 1, 0, 0, 1, 1],
    [5000, 2, 1, 1, 1, 0, 0, 1, 0, 0],
    [7000, 4, 3, 2, 0, 1, 1, 2, 1, 1],
    [6500, 3, 2, 2, 0, 1, 0, 1, 0, 1]
]


if best_model is not None:
    prediction = best_model.predict(data_to_predict)
    print("Best Model:", best_model_name)
    print("Predicted Price:", prediction)
else:
    print("No best model found. Something went wrong.")